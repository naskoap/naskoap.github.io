<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://naskoap.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://naskoap.github.io/" rel="alternate" type="text/html" /><updated>2021-09-17T00:23:16+00:00</updated><id>https://naskoap.github.io/feed.xml</id><title type="html">Nasko Apostolov</title><subtitle>© 2021  Nasko Apostolov. Powered by Jekyll.</subtitle><entry><title type="html">Improving CNN performance</title><link href="https://naskoap.github.io/2021/09/12/cnn-musings.html" rel="alternate" type="text/html" title="Improving CNN performance" /><published>2021-09-12T00:00:00+00:00</published><updated>2021-09-12T00:00:00+00:00</updated><id>https://naskoap.github.io/2021/09/12/cnn-musings</id><content type="html" xml:base="https://naskoap.github.io/2021/09/12/cnn-musings.html">&lt;p&gt;One of the main objectives of the &lt;i&gt;AI Trees project&lt;/i&gt; is to attain the optimal CNN performance. I trained instances 
of ResNet 50 and Inception V3 which allows us to compare &lt;i&gt; SafeTree &lt;/i&gt; with state-of-
the-art models. These new models, however, require fine-tuning in order to achieve better results.
The preliminary results point at approximately 67% validation accuracy. It is important to point
out the “Pr_Po_Im” scenario was used for training which typically leads to worse results given
the three-class problem. Additionally, I generated performance summary metrics and classification reports for the different archutectures.&lt;/p&gt;

&lt;p&gt;After training ResNet 50 and Inception V3 on multiple resolution types and scenarios, I discovered
that deep networks tend to require much longer training (more than 100 epochs) for convergence to
occur. Moreover, a three-class problem such as the Probable/Possible/Improbable classification still
represents a challenge even for a state-of the-art model. Partially, this could be attributed to the size of the input data set
as both Resnet and Inception typically require thousands of images for sufficient training.&lt;/p&gt;

&lt;p&gt;By visualizing the CNN performance I observed that training reaches its limit between the 60th and 100th epoch. Early Stopping is a way to arrive at the optimal epoch values earlier and also avoid overtraining the networks.&lt;/p&gt;</content><author><name></name></author><summary type="html">One of the main objectives of the AI Trees project is to attain the optimal CNN performance. I trained instances of ResNet 50 and Inception V3 which allows us to compare SafeTree with state-of- the-art models. These new models, however, require fine-tuning in order to achieve better results. The preliminary results point at approximately 67% validation accuracy. It is important to point out the “Pr_Po_Im” scenario was used for training which typically leads to worse results given the three-class problem. Additionally, I generated performance summary metrics and classification reports for the different archutectures.</summary></entry></feed>